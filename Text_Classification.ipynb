{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1lOzjLA2g5qmQ5ac-Osu95bZ6WGjS4fLz",
      "authorship_tag": "ABX9TyP4FWbrXZe1zTTsBKdj/WXC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chahatgarg884/Topsis2-102203557-Chahat_Garg/blob/main/Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVwUMtXJLJiE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "models = ['GPT-3', 'T5', 'ELECTRA', 'ERNIE', 'DeBERTa']\n",
        "performance_data = np.array([\n",
        "    [92, 89, 91, 160],  # GPT-3\n",
        "    [93, 91, 94, 180],  # T5\n",
        "    [88, 87, 89, 150],  # ELECTRA\n",
        "    [85, 84, 86, 130],  # ERNIE\n",
        "    [90, 88, 91, 140]   # DeBERTa\n",
        "])\n",
        "\n",
        "weights = np.array([0.35, 0.25, 0.2, 0.2])\n",
        "criteria = np.array([1, 1, 1, -1])\n",
        "\n",
        "normalized_data = performance_data / np.linalg.norm(performance_data, axis=0)\n",
        "\n",
        "weighted_performance = normalized_data * weights\n",
        "\n",
        "ideal_best = np.max(weighted_performance * (criteria == 1), axis=0) + np.min(weighted_performance * (criteria == -1), axis=0)\n",
        "ideal_worst = np.min(weighted_performance * (criteria == 1), axis=0) + np.max(weighted_performance * (criteria == -1), axis=0)\n",
        "\n",
        "distance_to_best = np.sqrt(np.sum((weighted_performance - ideal_best) ** 2, axis=1))\n",
        "distance_to_worst = np.sqrt(np.sum((weighted_performance - ideal_worst) ** 2, axis=1))\n",
        "\n",
        "topsis_scores = distance_to_worst / (distance_to_best + distance_to_worst)\n",
        "ranking_order = np.argsort(topsis_scores)[::-1]\n",
        "\n",
        "ranking_df = pd.DataFrame({'Model': models, 'TOPSIS Score': topsis_scores})\n",
        "ranking_df = ranking_df.iloc[ranking_order].reset_index(drop=True)\n",
        "print(ranking_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(ranking_df['Model'], ranking_df['TOPSIS Score'], color='lightcoral')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('TOPSIS Score')\n",
        "plt.title('TOPSIS Score Comparison Among Models')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "performance_df = pd.DataFrame(performance_data, columns=[\"Accuracy\", \"Precision\", \"Recall\", \"Inference Time\"], index=models)\n",
        "performance_df['TOPSIS Score'] = topsis_scores\n",
        "\n",
        "performance_df['Rank'] = performance_df['TOPSIS Score'].rank(ascending=False)\n",
        "\n",
        "performance_df = performance_df[['Accuracy', 'Precision', 'Recall', 'Inference Time', 'TOPSIS Score', 'Rank']]\n",
        "performance_df = performance_df.iloc[ranking_order].reset_index(drop=True)\n",
        "\n",
        "performance_df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NNCbYxG0LNCm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}